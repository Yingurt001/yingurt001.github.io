<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simulation-Based Inference: Neural Posterior Estimation - Ying Zhang</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-links">
                <a href="../index.html">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z"></path></svg>
                    Home
                </a>
                <!-- <a href="../blog.html" class="active">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M2 5a2 2 0 012-2h7a2 2 0 012 2v4a2 2 0 01-2 2H9l-3 3v-3H4a2 2 0 01-2-2V5z"></path><path d="M15 7v2a4 4 0 01-4 4H9.828l-1.766 1.767c.28.149.599.233.938.233h2l3 3v-3h2a2 2 0 002-2V9a2 2 0 00-2-2h-1z"></path></svg>
                    Blog
                </a> -->
            </div>
        </div>
    </nav>

    <div class="container">
        <main class="content blog-post">
            <a href="../blog.html" class="back-to-blog">â† Back to Blog</a>
            
            <article>
                <div class="blog-post-header">
                    <h1 class="blog-post-title">Simulation-Based Inference: Neural Posterior Estimation</h1>
                    <div class="blog-post-meta">
                        <span>ğŸ“… January 16, 2025</span>
                        <span>ğŸ·ï¸ SBI, Bayesian Inference, Research</span>
                    </div>
                </div>

                <div class="blog-post-content">
                    <h3>1. ä»€ä¹ˆæ˜¯Simulation-Based Inferenceï¼Ÿ</h3>
                    <p>
                        Simulation-Based Inference (SBI)ï¼Œä¹Ÿç§°ä¸ºLikelihood-Free Inferenceï¼Œæ˜¯ä¸€ç±»ç”¨äºä»è§‚æµ‹æ•°æ®æ¨æ–­æ¨¡å‹å‚æ•°çš„æ–¹æ³•ã€‚å½“æ¨¡å‹çš„ä¼¼ç„¶å‡½æ•°éš¾ä»¥è®¡ç®—æˆ–ä¸å¯ç”¨æ—¶ï¼ŒSBIæ–¹æ³•ç‰¹åˆ«æœ‰ç”¨ã€‚
                    </p>
                    <p>
                        ä¼ ç»Ÿçš„è´å¶æ–¯æ¨ç†éœ€è¦è®¡ç®—ä¼¼ç„¶å‡½æ•° <code>p(x|Î¸)</code>ï¼Œä½†åœ¨è®¸å¤šå¤æ‚æ¨¡å‹ä¸­ï¼ˆå¦‚ç”Ÿç‰©ç³»ç»Ÿã€ç‰©ç†æ¨¡æ‹Ÿç­‰ï¼‰ï¼Œè¿™ä¸ªä¼¼ç„¶å‡½æ•°æ˜¯éš¾ä»¥è§£æè®¡ç®—çš„ã€‚SBIé€šè¿‡æ¨¡æ‹Ÿæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
                    </p>

                    <h3>2. ä¸ºä»€ä¹ˆéœ€è¦SBIï¼Ÿ</h3>
                    <p>
                        è®¸å¤šç§‘å­¦å’Œå·¥ç¨‹é—®é¢˜æ¶‰åŠå¤æ‚çš„æ¨¡æ‹Ÿæ¨¡å‹ï¼š
                    </p>
                    <ul>
                        <li><strong>è®¡ç®—ç”Ÿç‰©å­¦</strong>ï¼šåŸºå› è°ƒæ§ç½‘ç»œã€è›‹ç™½è´¨æŠ˜å </li>
                        <li><strong>å¤©ä½“ç‰©ç†å­¦</strong>ï¼šå®‡å®™å­¦æ¨¡æ‹Ÿã€å¼•åŠ›æ³¢åˆ†æ</li>
                        <li><strong>æµè¡Œç—…å­¦</strong>ï¼šç–¾ç—…ä¼ æ’­æ¨¡å‹</li>
                        <li><strong>ç¥ç»ç§‘å­¦</strong>ï¼šç¥ç»å…ƒåŠ¨åŠ›å­¦æ¨¡å‹</li>
                    </ul>
                    <p>
                        è¿™äº›æ¨¡å‹é€šå¸¸åªèƒ½é€šè¿‡æ¨¡æ‹Ÿæ¥è¯„ä¼°ï¼Œæ— æ³•ç›´æ¥è®¡ç®—ä¼¼ç„¶å‡½æ•°ã€‚
                    </p>

                    <h3>3. SBIçš„åŸºæœ¬æ¡†æ¶</h3>
                    <p>
                        SBIçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
                    </p>
                    <ol>
                        <li>ä»å…ˆéªŒåˆ†å¸ƒ <code>p(Î¸)</code> ä¸­é‡‡æ ·å‚æ•°</li>
                        <li>ä½¿ç”¨è¿™äº›å‚æ•°è¿è¡Œæ¨¡æ‹Ÿï¼Œç”Ÿæˆæ•°æ® <code>x ~ p(x|Î¸)</code></li>
                        <li>å­¦ä¹ ä»è§‚æµ‹æ•°æ®åˆ°å‚æ•°çš„åéªŒæ˜ å°„</li>
                    </ol>

                    <h3>4. Neural Posterior Estimation (NPE)</h3>
                    <p>
                        Neural Posterior Estimationæ˜¯SBIçš„ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œç›´æ¥ä¼°è®¡å‚æ•°çš„åéªŒåˆ†å¸ƒ <code>p(Î¸|x)</code>ã€‚
                    </p>

                    <h4>4.1 NPEçš„åŸºæœ¬åŸç†</h4>
                    <p>
                        NPEè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œ <code>q_Ï†(Î¸|x)</code> æ¥è¿‘ä¼¼çœŸå®çš„åéªŒåˆ†å¸ƒ <code>p(Î¸|x)</code>ã€‚è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–ï¼š
                    </p>
                    <pre><code>E_{p(Î¸,x)}[log q_Ï†(Î¸|x)]

å…¶ä¸­ p(Î¸,x) = p(Î¸) p(x|Î¸)</code></pre>

                    <h4>4.2 è®­ç»ƒè¿‡ç¨‹</h4>
                    <pre><code>import torch
import torch.nn as nn
import numpy as np

class PosteriorEstimator(nn.Module):
    def __init__(self, data_dim, param_dim, hidden_dim=128):
        super(PosteriorEstimator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(data_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, param_dim * 2)  # å‡å€¼å’Œæ–¹å·®
        )
    
    def forward(self, x):
        output = self.net(x)
        mean = output[:, :self.param_dim]
        log_std = output[:, self.param_dim:]
        return mean, log_std
    
    def sample(self, x, n_samples=1000):
        mean, log_std = self.forward(x)
        std = torch.exp(log_std)
        samples = torch.normal(mean, std)
        return samples

def train_npe(simulator, prior, n_simulations=10000):
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    params = prior.sample((n_simulations,))
    data = simulator(params)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = PosteriorEstimator(data_dim=data.shape[1], 
                              param_dim=params.shape[1])
    optimizer = torch.optim.Adam(model.parameters())
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(1000):
        # éšæœºæ‰“ä¹±æ•°æ®
        indices = torch.randperm(n_simulations)
        params_shuffled = params[indices]
        data_shuffled = data[indices]
        
        # å‰å‘ä¼ æ’­
        mean, log_std = model(data_shuffled)
        std = torch.exp(log_std)
        
        # è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶
        dist = torch.distributions.Normal(mean, std)
        loss = -dist.log_prob(params_shuffled).mean()
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
    
    return model</code></pre>

                    <h3>5. æ¡ä»¶å½’ä¸€åŒ–æµï¼ˆConditional Normalizing Flowsï¼‰</h3>
                    <p>
                        æ›´é«˜çº§çš„NPEæ–¹æ³•ä½¿ç”¨æ¡ä»¶å½’ä¸€åŒ–æµæ¥å»ºæ¨¡å¤æ‚çš„åéªŒåˆ†å¸ƒã€‚å½’ä¸€åŒ–æµèƒ½å¤Ÿè¡¨ç¤ºä»»æ„å¤æ‚çš„æ¦‚ç‡åˆ†å¸ƒã€‚
                    </p>

                    <h4>5.1 å½’ä¸€åŒ–æµåŸºç¡€</h4>
                    <p>
                        å½’ä¸€åŒ–æµé€šè¿‡ä¸€ç³»åˆ—å¯é€†å˜æ¢å°†ä¸€ä¸ªç®€å•åˆ†å¸ƒï¼ˆå¦‚æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰è½¬æ¢ä¸ºå¤æ‚åˆ†å¸ƒï¼š
                    </p>
                    <pre><code>class ConditionalFlow(nn.Module):
    def __init__(self, data_dim, param_dim, n_transforms=5):
        super(ConditionalFlow, self).__init__()
        self.transforms = nn.ModuleList([
            AffineCouplingLayer(data_dim, param_dim) 
            for _ in range(n_transforms)
        ])
    
    def forward(self, params, data):
        log_det = 0
        z = params
        for transform in self.transforms:
            z, ld = transform(z, data)
            log_det += ld
        return z, log_det
    
    def inverse(self, z, data):
        log_det = 0
        params = z
        for transform in reversed(self.transforms):
            params, ld = transform.inverse(params, data)
            log_det += ld
        return params, log_det</code></pre>

                    <h3>6. å®é™…åº”ç”¨ç¤ºä¾‹</h3>
                    <h4>6.1 ç®€å•ç¤ºä¾‹ï¼šé«˜æ–¯æ¨¡å‹</h4>
                    <p>
                        å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼šè§‚æµ‹æ•°æ®æ¥è‡ªé«˜æ–¯åˆ†å¸ƒï¼Œæˆ‘ä»¬æƒ³æ¨æ–­å‡å€¼å’Œæ ‡å‡†å·®ã€‚
                    </p>
                    <pre><code>def gaussian_simulator(params, n_obs=100):
    """æ¨¡æ‹Ÿå™¨ï¼šä»é«˜æ–¯åˆ†å¸ƒç”Ÿæˆæ•°æ®"""
    mean, std = params[:, 0], params[:, 1]
    data = []
    for m, s in zip(mean, std):
        samples = np.random.normal(m, s, n_obs)
        # ä½¿ç”¨æ‘˜è¦ç»Ÿè®¡é‡
        summary = np.array([np.mean(samples), np.std(samples)])
        data.append(summary)
    return np.array(data)

# å®šä¹‰å…ˆéªŒ
from scipy.stats import uniform, gamma
prior_mean = uniform(loc=-5, scale=10)  # å‡å€¼åœ¨[-5, 5]
prior_std = gamma(a=2, scale=1)  # æ ‡å‡†å·®

# è®­ç»ƒNPEæ¨¡å‹
model = train_npe(gaussian_simulator, prior, n_simulations=50000)

# å¯¹æ–°è§‚æµ‹æ•°æ®è¿›è¡Œæ¨ç†
observed_data = np.array([[2.5, 1.2]])  # è§‚æµ‹åˆ°çš„å‡å€¼å’Œæ ‡å‡†å·®
posterior_samples = model.sample(torch.tensor(observed_data, dtype=torch.float32))
print(f"åéªŒå‡å€¼ä¼°è®¡: {posterior_samples.mean(axis=0)}")</code></pre>

                    <h3>7. è¯„ä¼°å’ŒéªŒè¯</h3>
                    <p>
                        è¯„ä¼°NPEæ¨¡å‹è´¨é‡çš„æ–¹æ³•ï¼š
                    </p>
                    <ul>
                        <li><strong>åéªŒé¢„æµ‹æ£€æŸ¥</strong>ï¼šä»åéªŒé‡‡æ ·å‚æ•°ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦ä¸è§‚æµ‹æ•°æ®ä¸€è‡´</li>
                        <li><strong>æ ¡å‡†æ£€æŸ¥</strong>ï¼šæ£€æŸ¥åéªŒåˆ†ä½æ•°çš„æ ¡å‡†</li>
                        <li><strong>è¦†ç›–ç‡</strong>ï¼šæ£€æŸ¥çœŸå®å‚æ•°æ˜¯å¦è½åœ¨åéªŒç½®ä¿¡åŒºé—´å†…</li>
                    </ul>

                    <h3>8. æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ</h3>
                    <h4>8.1 é«˜ç»´æ•°æ®</h4>
                    <p>
                        å½“æ•°æ®ç»´åº¦å¾ˆé«˜æ—¶ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®å¯èƒ½ä¸é«˜æ•ˆã€‚è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨æ‘˜è¦ç»Ÿè®¡é‡ï¼ˆSummary Statisticsï¼‰æˆ–å­¦ä¹ æ‘˜è¦ç½‘ç»œã€‚
                    </p>

                    <h4>8.2 æ¨¡æ‹Ÿæˆæœ¬</h4>
                    <p>
                        å¦‚æœæ¨¡æ‹Ÿéå¸¸è€—æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ï¼š
                    </p>
                    <ul>
                        <li>ä¸»åŠ¨å­¦ä¹ ï¼šé€‰æ‹©æœ€æœ‰ä¿¡æ¯é‡çš„å‚æ•°è¿›è¡Œæ¨¡æ‹Ÿ</li>
                        <li>æ¨¡æ‹Ÿå™¨æ›¿ä»£ï¼šè®­ç»ƒç¥ç»ç½‘ç»œæ›¿ä»£æ˜‚è´µçš„æ¨¡æ‹Ÿå™¨</li>
                    </ul>

                    <h3>9. ç ”ç©¶å±•æœ›</h3>
                    <p>
                        SBIå’ŒNPEæ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œæœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬ï¼š
                    </p>
                    <ul>
                        <li>æ›´é«˜æ•ˆçš„å½’ä¸€åŒ–æµæ¶æ„</li>
                        <li>å¤„ç†é«˜ç»´å‚æ•°ç©ºé—´çš„æ–¹æ³•</li>
                        <li>ç»“åˆé¢†åŸŸçŸ¥è¯†çš„çº¦æŸ</li>
                        <li>ä¸ç¡®å®šæ€§é‡åŒ–</li>
                    </ul>

                    <h3>10. æ€»ç»“</h3>
                    <p>
                        Simulation-Based Inferenceå’ŒNeural Posterior Estimationä¸ºå¤„ç†å¤æ‚æ¨¡å‹æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚å½“ä¼ ç»Ÿæ–¹æ³•æ— æ³•åº”ç”¨æ—¶ï¼ŒSBIä½¿æˆ‘ä»¬èƒ½å¤Ÿä»è§‚æµ‹æ•°æ®ä¸­æ¨æ–­æ¨¡å‹å‚æ•°ï¼Œè¿™å¯¹äºè®¸å¤šç§‘å­¦å’Œå·¥ç¨‹é—®é¢˜è‡³å…³é‡è¦ã€‚
                    </p>
                    <p>
                        åœ¨æˆ‘çš„EPSRCèµ„åŠ©é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬æ­£åœ¨æ¢ç´¢è¿™äº›æ–¹æ³•åœ¨ç»Ÿè®¡å»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œå¸Œæœ›èƒ½ä¸ºè¿™ä¸€é¢†åŸŸåšå‡ºè´¡çŒ®ã€‚
                    </p>

                    <blockquote>
                        <strong>æ¨èèµ„æºï¼š</strong>
                        <ul>
                            <li>sbi library: PythonåŒ…ï¼Œæä¾›äº†SBIçš„å®Œæ•´å®ç°</li>
                            <li>Papamakarios et al. (2019): "Neural Likelihood Estimation"</li>
                            <li>Greenberg et al. (2019): "Automatic Posterior Transformation for Likelihood-Free Inference"</li>
                        </ul>
                    </blockquote>
                </div>
            </article>
        </main>
    </div>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-links">
                <a href="#" title="GitHub"><img src="https://img.icons8.com/color/24/000000/github.png" alt="GitHub"></a>
            </div>
            <div class="footer-copyright">Â© Ying Zhang</div>
        </div>
    </footer>
</body>
</html>

