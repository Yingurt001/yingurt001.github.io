<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning Applications in Credit Risk Analysis - Ying Zhang</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-links">
                <a href="../index.html">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z"></path></svg>
                    Home
                </a>
                <!-- <a href="../blog.html" class="active">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M2 5a2 2 0 012-2h7a2 2 0 012 2v4a2 2 0 01-2 2H9l-3 3v-3H4a2 2 0 01-2-2V5z"></path><path d="M15 7v2a4 4 0 01-4 4H9.828l-1.766 1.767c.28.149.599.233.938.233h2l3 3v-3h2a2 2 0 002-2V9a2 2 0 00-2-2h-1z"></path></svg>
                    Blog
                </a> -->
            </div>
        </div>
    </nav>

    <div class="container">
        <main class="content blog-post">
            <a href="../blog.html" class="back-to-blog">â† Back to Blog</a>
            
            <article>
                <div class="blog-post-header">
                    <h1 class="blog-post-title">Machine Learning Applications in Credit Risk Analysis</h1>
                    <div class="blog-post-meta">
                        <span>ğŸ“… January 18, 2025</span>
                        <span>ğŸ·ï¸ Credit Risk, Time Series, KAN Networks</span>
                    </div>
                </div>

                <div class="blog-post-content">
                    <h3>1. å¼•è¨€</h3>
                    <p>
                        ä¿¡ç”¨é£é™©åˆ†ææ˜¯é‡‘èé¢†åŸŸçš„é‡è¦é—®é¢˜ã€‚ä¼ ç»Ÿçš„ç»Ÿè®¡æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„éçº¿æ€§å…³ç³»æ—¶å­˜åœ¨å±€é™æ€§ï¼Œè€Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿä»å¤§é‡å†å²æ•°æ®ä¸­å­¦ä¹ å¤æ‚çš„æ¨¡å¼ï¼Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚
                    </p>
                    <p>
                        æœ¬æ–‡åŸºäºæˆ‘åœ¨Applied Soft Computingå’ŒCSCRä¼šè®®ä¸Šçš„ç ”ç©¶å·¥ä½œï¼Œåˆ†äº«ä½¿ç”¨KANç½‘ç»œã€LSTMã€GRUç­‰æ¨¡å‹è¿›è¡Œä¿¡ç”¨é£é™©é¢„æµ‹çš„ç»éªŒã€‚
                    </p>

                    <h3>2. é—®é¢˜å®šä¹‰</h3>
                    <p>
                        ä¿¡ç”¨é£é™©é¢„æµ‹çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ï¼š
                    </p>
                    <ul>
                        <li><strong>æ—©æœŸé¢„æµ‹</strong>ï¼šåœ¨è´·æ¬¾è¿çº¦å‘ç”Ÿä¹‹å‰é¢„æµ‹é£é™©</li>
                        <li><strong>åè´·æ¬¾æ£€æµ‹</strong>ï¼šç›‘æ§å·²å‘æ”¾è´·æ¬¾çš„è¿çº¦é£é™©</li>
                        <li><strong>é£é™©è¯„ä¼°</strong>ï¼šé‡åŒ–å®¢æˆ·çš„ä¿¡ç”¨é£é™©ç­‰çº§</li>
                    </ul>

                    <h3>3. æ•°æ®é¢„å¤„ç†</h3>
                    <h4>3.1 æ•°æ®æ¸…æ´—</h4>
                    <pre><code>import pandas as pd
import numpy as np

def clean_data(df):
    # å¤„ç†ç¼ºå¤±å€¼
    df = df.dropna(subset=['target'])
    
    # å¤„ç†å¼‚å¸¸å€¼
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]
    
    # å¤„ç†ç±»åˆ«å˜é‡
    categorical_cols = df.select_dtypes(include=['object']).columns
    df = pd.get_dummies(df, columns=categorical_cols)
    
    return df</code></pre>

                    <h4>3.2 ç‰¹å¾å·¥ç¨‹</h4>
                    <p>
                        å¯¹äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºæ—¶é—´ç‰¹å¾ï¼š
                    </p>
                    <ul>
                        <li>æ»åç‰¹å¾ï¼ˆLag Featuresï¼‰</li>
                        <li>æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ï¼ˆRolling Statisticsï¼‰</li>
                        <li>æ—¶é—´ç‰¹å¾ï¼ˆå¹´ã€æœˆã€æ—¥ç­‰ï¼‰</li>
                        <li>äº¤äº’ç‰¹å¾ï¼ˆç‰¹å¾ä¹‹é—´çš„ç»„åˆï¼‰</li>
                    </ul>

                    <h3>4. æ¨¡å‹æ¶æ„</h3>
                    <h4>4.1 KANç½‘ç»œï¼ˆKolmogorov-Arnold Networksï¼‰</h4>
                    <p>
                        KANç½‘ç»œæ˜¯ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„æ¿€æ´»å‡½æ•°æ›¿ä»£ä¼ ç»Ÿçš„å›ºå®šæ¿€æ´»å‡½æ•°ã€‚åœ¨ä¿¡ç”¨é£é™©é¢„æµ‹ä¸­ï¼ŒKANç½‘ç»œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ•°æ®çš„éçº¿æ€§å…³ç³»ã€‚
                    </p>
                    <pre><code>import torch
import torch.nn as nn

class KANLayer(nn.Module):
    def __init__(self, in_features, out_features, grid_size=5):
        super(KANLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.grid_size = grid_size
        
        # å¯å­¦ä¹ çš„Bæ ·æ¡åŸºå‡½æ•°ç³»æ•°
        self.coeff = nn.Parameter(torch.randn(in_features, out_features, grid_size))
        
    def forward(self, x):
        # Bæ ·æ¡åŸºå‡½æ•°æ’å€¼
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…å®ç°æ›´å¤æ‚
        output = torch.zeros(x.shape[0], self.out_features)
        for i in range(self.in_features):
            for j in range(self.out_features):
                # ä½¿ç”¨Bæ ·æ¡åŸºå‡½æ•°è¿›è¡Œæ’å€¼
                basis = self.bspline_basis(x[:, i])
                output[:, j] += torch.sum(self.coeff[i, j] * basis, dim=1)
        return output</code></pre>

                    <h4>4.2 LSTM/GRUæ¨¡å‹</h4>
                    <p>
                        å¯¹äºæ—¶é—´åºåˆ—æ•°æ®ï¼ŒLSTMå’ŒGRUèƒ½å¤Ÿæ•æ‰é•¿æœŸä¾èµ–å…³ç³»ï¼š
                    </p>
                    <pre><code>class CreditRiskLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(CreditRiskLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # x shape: (batch, seq_len, input_size)
        lstm_out, _ = self.lstm(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]
        output = self.fc(last_output)
        return self.sigmoid(output)</code></pre>

                    <h4>4.3 ResE-BiLSTMæ¨¡å‹</h4>
                    <p>
                        åŒå‘LSTMç»“åˆæ®‹å·®è¿æ¥ï¼Œèƒ½å¤ŸåŒæ—¶è€ƒè™‘å‰å‘å’Œåå‘çš„æ—¶é—´ä¾èµ–ï¼š
                    </p>
                    <pre><code>class ResE_BiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(ResE_BiLSTM, self).__init__()
        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers,
                              batch_first=True, bidirectional=True)
        self.residual = nn.Linear(input_size, hidden_size * 2)
        self.fc = nn.Linear(hidden_size * 2, 1)
    
    def forward(self, x):
        bilstm_out, _ = self.bilstm(x)
        residual = self.residual(x[:, -1, :])
        output = bilstm_out[:, -1, :] + residual
        return torch.sigmoid(self.fc(output))</code></pre>

                    <h3>5. æ¨¡å‹è®­ç»ƒ</h3>
                    <h4>5.1 è®­ç»ƒç­–ç•¥</h4>
                    <ul>
                        <li>ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯</li>
                        <li>å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼ˆä½¿ç”¨åŠ æƒæŸå¤±æˆ–SMOTEï¼‰</li>
                        <li>æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ</li>
                        <li>å­¦ä¹ ç‡è°ƒåº¦</li>
                    </ul>

                    <h4>5.2 è®­ç»ƒä»£ç ç¤ºä¾‹</h4>
                    <pre><code>def train_model(model, train_loader, val_loader, num_epochs=100):
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')
    
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
        
        scheduler.step(val_loss)
        
        # æ—©åœæ£€æŸ¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'best_model.pth')
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break
        
        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")</code></pre>

                    <h3>6. æ¨¡å‹è¯„ä¼°</h3>
                    <p>
                        ä¿¡ç”¨é£é™©é¢„æµ‹å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼š
                    </p>
                    <ul>
                        <li><strong>AUC-ROC</strong>ï¼šè¯„ä¼°æ•´ä½“åˆ†ç±»æ€§èƒ½</li>
                        <li><strong>ç²¾ç¡®ç‡å’Œå¬å›ç‡</strong>ï¼šå¹³è¡¡è¯¯æŠ¥å’Œæ¼æŠ¥</li>
                        <li><strong>F1-Score</strong>ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡</li>
                        <li><strong>KSç»Ÿè®¡é‡</strong>ï¼šè¯„ä¼°æ¨¡å‹åŒºåˆ†èƒ½åŠ›</li>
                    </ul>

                    <h3>7. å®éªŒç»“æœ</h3>
                    <p>
                        åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼ŒKANç½‘ç»œå’ŒLSTM/GRUæ¨¡å‹åœ¨ä¿¡ç”¨é£é™©é¢„æµ‹ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼š
                    </p>
                    <ul>
                        <li>KANç½‘ç»œåœ¨æ•æ‰éçº¿æ€§å…³ç³»æ–¹é¢è¡¨ç°ä¼˜å¼‚</li>
                        <li>LSTM/GRUåœ¨å¤„ç†æ—¶é—´åºåˆ—ä¾èµ–æ–¹é¢æ•ˆæœæ˜¾è‘—</li>
                        <li>ResE-BiLSTMç»“åˆäº†åŒå‘ä¿¡æ¯å’Œæ®‹å·®è¿æ¥ï¼Œæ€§èƒ½æœ€ä½³</li>
                    </ul>

                    <h3>8. å®é™…åº”ç”¨å»ºè®®</h3>
                    <ul>
                        <li>æ•°æ®è´¨é‡è‡³å…³é‡è¦ï¼Œéœ€è¦ä»”ç»†è¿›è¡Œæ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹</li>
                        <li>è€ƒè™‘æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œä½¿ç”¨SHAPå€¼ç­‰æ–¹æ³•</li>
                        <li>å®šæœŸé‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œé€‚åº”æ•°æ®åˆ†å¸ƒçš„å˜åŒ–</li>
                        <li>ç»“åˆä¸šåŠ¡çŸ¥è¯†ï¼Œä¸è¦å®Œå…¨ä¾èµ–æ¨¡å‹</li>
                        <li>æ³¨æ„æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œåˆè§„æ€§è¦æ±‚</li>
                    </ul>

                    <h3>9. æ€»ç»“</h3>
                    <p>
                        æœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨ä¿¡ç”¨é£é™©åˆ†æä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚é€šè¿‡åˆç†é€‰æ‹©æ¨¡å‹æ¶æ„ã€ç²¾å¿ƒè®¾è®¡ç‰¹å¾å·¥ç¨‹å’Œè®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºå‡ºé«˜æ€§èƒ½çš„ä¿¡ç”¨é£é™©é¢„æµ‹ç³»ç»Ÿã€‚æœªæ¥ï¼Œéšç€æ›´å¤šå…ˆè¿›æ¨¡å‹çš„å‡ºç°ï¼Œè¿™ä¸€é¢†åŸŸè¿˜æœ‰å¾ˆå¤§çš„å‘å±•ç©ºé—´ã€‚
                    </p>

                    <blockquote>
                        <strong>ç›¸å…³è®ºæ–‡ï¼š</strong>
                        <ul>
                            <li>Kolmogorovâ€“Arnold Networks-based GRU and LSTM for Loan Default Early Prediction (Applied Soft Computing, 2025, Under Review)</li>
                            <li>Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection (CSCR III, 2024, Accepted)</li>
                        </ul>
                    </blockquote>
                </div>
            </article>
        </main>
    </div>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-links">
                <a href="#" title="GitHub"><img src="https://img.icons8.com/color/24/000000/github.png" alt="GitHub"></a>
            </div>
            <div class="footer-copyright">Â© Ying Zhang</div>
        </div>
    </footer>
</body>
</html>

