<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Networks Basics: From Perceptron to Multi-Layer Perceptron - Ying Zhang</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-links">
                <a href="../index.html">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z"></path></svg>
                    Home
                </a>
                <!-- <a href="../blog.html" class="active">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M2 5a2 2 0 012-2h7a2 2 0 012 2v4a2 2 0 01-2 2H9l-3 3v-3H4a2 2 0 01-2-2V5z"></path><path d="M15 7v2a4 4 0 01-4 4H9.828l-1.766 1.767c.28.149.599.233.938.233h2l3 3v-3h2a2 2 0 002-2V9a2 2 0 00-2-2h-1z"></path></svg>
                    Blog
                </a> -->
            </div>
        </div>
    </nav>

    <div class="container">
        <main class="content blog-post">
            <a href="../blog.html" class="back-to-blog">â† Back to Blog</a>
            
            <article>
                <div class="blog-post-header">
                    <h1 class="blog-post-title">Neural Networks Basics: From Perceptron to Multi-Layer Perceptron</h1>
                    <div class="blog-post-meta">
                        <span>ğŸ“… January 20, 2025</span>
                        <span>ğŸ·ï¸ Neural Networks, Deep Learning, Python</span>
                    </div>
                </div>

                <div class="blog-post-content">
                    <h3>1. What are Neural Networks?</h3>
                    <p>
                        Neural networks are computational models inspired by biological neurons, consisting of a large number of interconnected nodes (neurons). Each neuron receives inputs, performs weighted summation, and produces output through an activation function. Neural networks learn patterns in data by adjusting weights and biases.
                    </p>

                    <h3>2. Perceptron</h3>
                    <p>
                        The perceptron is the simplest neural network model, proposed by Frank Rosenblatt in 1957. It can only solve linearly separable problems.
                    </p>

                    <h4>2.1 Structure of Perceptron</h4>
                    <p>
                        A perceptron consists of:
                    </p>
                    <ul>
                        <li><strong>Input Layer</strong>: Receives feature vectors</li>
                        <li><strong>Weights</strong>: Each input has a weight</li>
                        <li><strong>Bias</strong>: A constant term</li>
                        <li><strong>Activation Function</strong>: Usually a step function or sign function</li>
                    </ul>

                    <h4>2.2 Mathematical Representation</h4>
                    <p>
                        For input vector <code>x = [xâ‚, xâ‚‚, ..., xâ‚™]</code>, the perceptron output is:
                    </p>
                    <pre><code>y = f(Î£(wáµ¢ Ã— xáµ¢) + b)

where:
- wáµ¢ is the weight of the i-th input
- b is the bias
- f is the activation function</code></pre>

                    <h4>2.3 Pythonå®ç°æ„ŸçŸ¥æœº</h4>
                    <pre><code>import numpy as np

class Perceptron:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
    
    def fit(self, X, y):
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        self.weights = np.zeros(X.shape[1])
        self.bias = 0
        
        # è®­ç»ƒè¿‡ç¨‹
        for _ in range(self.n_iterations):
            for i in range(X.shape[0]):
                # è®¡ç®—é¢„æµ‹å€¼
                linear_output = np.dot(X[i], self.weights) + self.bias
                prediction = self.activation(linear_output)
                
                # æ›´æ–°æƒé‡å’Œåç½®
                update = self.learning_rate * (y[i] - prediction)
                self.weights += update * X[i]
                self.bias += update
    
    def activation(self, x):
        return 1 if x >= 0 else 0
    
    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        return np.array([self.activation(x) for x in linear_output])</code></pre>

                    <h3>3. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰</h3>
                    <p>
                        å¤šå±‚æ„ŸçŸ¥æœºæ˜¯æ„ŸçŸ¥æœºçš„æ‰©å±•ï¼ŒåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚ã€‚è¿™ä½¿å¾—MLPèƒ½å¤Ÿå­¦ä¹ éçº¿æ€§å…³ç³»ï¼Œè§£å†³æ›´å¤æ‚çš„é—®é¢˜ã€‚
                    </p>

                    <h4>3.1 MLPçš„ç»“æ„</h4>
                    <ul>
                        <li><strong>è¾“å…¥å±‚</strong>ï¼šæ¥æ”¶åŸå§‹ç‰¹å¾</li>
                        <li><strong>éšè—å±‚</strong>ï¼šä¸€ä¸ªæˆ–å¤šä¸ªä¸­é—´å±‚ï¼Œè¿›è¡Œç‰¹å¾å˜æ¢</li>
                        <li><strong>è¾“å‡ºå±‚</strong>ï¼šäº§ç”Ÿæœ€ç»ˆé¢„æµ‹</li>
                    </ul>

                    <h4>3.2 å‰å‘ä¼ æ’­</h4>
                    <p>
                        å‰å‘ä¼ æ’­æ˜¯æ•°æ®ä»è¾“å…¥å±‚æµå‘è¾“å‡ºå±‚çš„è¿‡ç¨‹ï¼š
                    </p>
                    <pre><code>def forward_propagation(X, weights, biases, activation_func):
    """
    X: è¾“å…¥æ•°æ®
    weights: æƒé‡åˆ—è¡¨ [W1, W2, ..., Wn]
    biases: åç½®åˆ—è¡¨ [b1, b2, ..., bn]
    activation_func: æ¿€æ´»å‡½æ•°
    """
    activations = [X]
    
    for W, b in zip(weights, biases):
        z = np.dot(activations[-1], W) + b
        a = activation_func(z)
        activations.append(a)
    
    return activations</code></pre>

                    <h4>3.3 åå‘ä¼ æ’­ç®—æ³•</h4>
                    <p>
                        åå‘ä¼ æ’­æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ï¼Œé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å¯¹æƒé‡çš„æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ã€‚
                    </p>
                    <p>
                        åŸºæœ¬æ­¥éª¤ï¼š
                    </p>
                    <ol>
                        <li>å‰å‘ä¼ æ’­è®¡ç®—è¾“å‡ºå’ŒæŸå¤±</li>
                        <li>è®¡ç®—è¾“å‡ºå±‚çš„è¯¯å·®</li>
                        <li>å°†è¯¯å·®åå‘ä¼ æ’­åˆ°éšè—å±‚</li>
                        <li>è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°æƒé‡</li>
                    </ol>

                    <h4>3.4 ä½¿ç”¨PyTorchå®ç°MLP</h4>
                    <pre><code>import torch
import torch.nn as nn
import torch.optim as optim

class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.sigmoid(out)
        return out

# ä½¿ç”¨ç¤ºä¾‹
model = MLP(input_size=784, hidden_size=128, output_size=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for batch_x, batch_y in train_loader:
        # å‰å‘ä¼ æ’­
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</code></pre>

                    <h3>4. æ¿€æ´»å‡½æ•°</h3>
                    <p>
                        æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚æ¨¡å¼ã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ï¼š
                    </p>
                    <ul>
                        <li><strong>Sigmoid</strong>ï¼š<code>Ïƒ(x) = 1 / (1 + eâ»Ë£)</code>ï¼Œè¾“å‡ºèŒƒå›´(0,1)</li>
                        <li><strong>Tanh</strong>ï¼š<code>tanh(x) = (eË£ - eâ»Ë£) / (eË£ + eâ»Ë£)</code>ï¼Œè¾“å‡ºèŒƒå›´(-1,1)</li>
                        <li><strong>ReLU</strong>ï¼š<code>ReLU(x) = max(0, x)</code>ï¼Œæœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°</li>
                        <li><strong>Leaky ReLU</strong>ï¼šè§£å†³ReLUçš„"æ­»äº¡ç¥ç»å…ƒ"é—®é¢˜</li>
                    </ul>

                    <h3>5. æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨</h3>
                    <h4>5.1 å¸¸ç”¨æŸå¤±å‡½æ•°</h4>
                    <ul>
                        <li><strong>å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰</strong>ï¼šç”¨äºå›å½’é—®é¢˜</li>
                        <li><strong>äº¤å‰ç†µæŸå¤±</strong>ï¼šç”¨äºåˆ†ç±»é—®é¢˜</li>
                        <li><strong>äºŒå…ƒäº¤å‰ç†µ</strong>ï¼šç”¨äºäºŒåˆ†ç±»é—®é¢˜</li>
                    </ul>

                    <h4>5.2 ä¼˜åŒ–å™¨</h4>
                    <ul>
                        <li><strong>éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰</strong>ï¼šåŸºç¡€ä¼˜åŒ–å™¨</li>
                        <li><strong>Adam</strong>ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œæœ€å¸¸ç”¨</li>
                        <li><strong>RMSprop</strong>ï¼šé€‚åˆå¤„ç†éå¹³ç¨³ç›®æ ‡</li>
                    </ul>

                    <h3>6. å®è·µå»ºè®®</h3>
                    <ul>
                        <li>ä»ç®€å•æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦</li>
                        <li>ä½¿ç”¨åˆé€‚çš„æ¿€æ´»å‡½æ•°å’Œåˆå§‹åŒ–æ–¹æ³•</li>
                        <li>æ³¨æ„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯</li>
                        <li>åˆç†è®¾ç½®å­¦ä¹ ç‡å’Œæ‰¹æ¬¡å¤§å°</li>
                        <li>ä½¿ç”¨éªŒè¯é›†ç›‘æ§æ¨¡å‹æ€§èƒ½</li>
                    </ul>

                    <h3>7. æ€»ç»“</h3>
                    <p>
                        ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºçŸ³ã€‚ä»ç®€å•çš„æ„ŸçŸ¥æœºåˆ°å¤æ‚çš„å¤šå±‚æ„ŸçŸ¥æœºï¼Œç†è§£è¿™äº›åŸºç¡€æ¦‚å¿µå¯¹äºæ·±å…¥å­¦ä¹ æ·±åº¦å­¦ä¹ è‡³å…³é‡è¦ã€‚é€šè¿‡å®è·µå’Œé¡¹ç›®ï¼Œä½ ä¼šé€æ¸æŒæ¡å¦‚ä½•è®¾è®¡å’Œè®­ç»ƒæœ‰æ•ˆçš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚
                    </p>

                    <blockquote>
                        <strong>æç¤ºï¼š</strong>å»ºè®®ä½¿ç”¨ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚PyTorchã€TensorFlowï¼‰æ¥å®ç°ç¥ç»ç½‘ç»œï¼Œå®ƒä»¬æä¾›äº†è‡ªåŠ¨å¾®åˆ†ã€GPUåŠ é€Ÿç­‰å¼ºå¤§åŠŸèƒ½ï¼Œå¯ä»¥å¤§å¤§æé«˜å¼€å‘æ•ˆç‡ã€‚
                    </blockquote>
                </div>
            </article>
        </main>
    </div>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-links">
                <a href="#" title="GitHub"><img src="https://img.icons8.com/color/24/000000/github.png" alt="GitHub"></a>
            </div>
            <div class="footer-copyright">Â© Ying Zhang</div>
        </div>
    </footer>
</body>
</html>

